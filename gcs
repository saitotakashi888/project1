# coding: utf-8
# GCS操作
'''
https://googleapis.dev/python/storage/latest/client.html
module\gcp\op_gcs.py
pip install --upgrade google-cloud-storage

cp
gsutil cp -r gs://bucket/model gs://bucket/backup/model/
gsutil cp -r gs://bucket/data gs://bucket/backup/data

'''
from mypool import MyPool

import configparser
import os
import re
import shutil
import subprocess
from io import StringIO
import multiprocessing
from boto.gs.connection import GSConnection
from error_module import *
import datetime as dt
from op_file import *
import pandas as pd
import io

from google.cloud import storage

from google.oauth2 import service_account
import gzip

# config-------------------------------------------------------------
inifile = configparser.ConfigParser()
inifile.read(os.environ.get('PATH_SCRIPT_ROOT') + '/conf/conf.ini')
key_file_path = os.environ.get('PATH_SCRIPT_ROOT') + '/' + inifile.get("common", "GOOGLE_APPLICATION_CREDENTIALS")

# Instantiates a client
default_project = inifile.get("bq", "project")
credentials = service_account.Credentials.from_service_account_file(key_file_path)

# gcs
access_key = inifile.get("gcs", "access_key")
secret_access_key = inifile.get("gcs", "secret_access_key")


class ApiExecutionError(Exception):
    """API実行エラーの例外クラス"""
    pass


class OpGcs:

    def __init__(self, project, bucket_name):
        self.project = project
        self.bucket_name = bucket_name

    def client(self):
        # プロジェクト名を指定してclientを作成
        client = storage.Client(
            project=self.project,
            credentials=credentials
        )
        return client

    # connect google cloud storage
    def gcs_con(self):
        gcs_con = GSConnection(access_key, secret_access_key)
        return gcs_con

    def get_bucket(self, bucket):
        gcs_con = self.gcs_con()
        return gcs_con.get_bucket(bucket)

    # GCS間でのファイルの移動
    def movefile(self, from_file, to_file):
        io = StringIO()
        bucket = self.get_bucket(self.bucket_name)
        try:
            bucket.get_key().get_file(from_file)
            io.seek(0)
            key = bucket.new_key(key_name=to_file)
            key.set_contents_from_file(io, replace=True)  # replaceは上書きの許可
        finally:
            io.close()
            return True if key else False

    def upload_file(self, local_file, gs_save_dir='', gs_save_filename=None):
        '''
        ファイルのアップロード gsutilはpython2系が必要
        アップロード先をディレクトリパスかファイルパスで指定
        :param string local_file: 元ファイル ワイルドカード指定が可能
        :param string gs_save_dir: アップロード先dir gs://bucket_name/aaaa
        :param string gs_save_filename: アップロード先ファイル
        :return:
        '''
        gs_save_dir = gs_save_dir.rstrip("/")  # 末尾のスラッシュを削除
        if gs_save_dir:  # アップロード先をディレクトリ指定
            cmd = 'gsutil -m cp -r "%s" "%s/"' % (local_file, gs_save_dir)
        elif gs_save_filename:
            cmd = 'gsutil -m cp "%s" "%s"' % (local_file, gs_save_filename)
        r = subprocess.getoutput(cmd)

        if not re.search('Operation completed', r):
            raise ApiExecutionError(r)
        return True

    def df_to_gcs(self, df, gcs_save_dir, prefix='', gs_save_filename=None, compression=True, file_type='tsv'):
        '''
        ファイルのアップロード gsutilはpython2系が必要
        ファイル名 = gs_save_filename if gs_save_filename else prefix_タイムスタンプ.tsv
        :param string df:
        :param gcs_save_dir: 保存ディレクトリ "gs://keep28day/work/"
        :param string prefix: 保存ファイルprefix gs_save_filenameがない場合のみ有効
        :param string gcs_save_dir: アップロード先dir
        :param string gs_save_filename: ファイル名 None:prefix_タイムスタンプ.tsv
        :param string compression: True:圧縮
        :return:
        '''

        gcs_save_dir = self.add_tail(gcs_save_dir, '/')
        path_save_local_dir = '/tmp/df_to_gcs/'
        gz = '.gz' if compression else ''
        path_save_fname = gs_save_filename if gs_save_filename else '{prefix}_{date}.{file_type}'.format(prefix=prefix,
                                                                                                         date=dt.datetime.now().strftime("%Y%m%d_%H%M%S"),
                                                                                                         file_type=file_type)
        gcs_save_fname = '{gcs_save_dir}{path_save_fname}'.format(gcs_save_dir=gcs_save_dir,
                                                                  path_save_fname=path_save_fname + gz)

        create_dir(path_save_local_dir)

        # 同名のファイルが存在する場合は削除
        self.delete_object(path_save_local_dir + path_save_fname)
        self.delete_object(path_save_local_dir + path_save_fname + gz)

        if file_type == 'json':
            # dfをjsonに書き出し
            df.to_json(path_save_local_dir + path_save_fname, orient='records', lines=True, date_format='iso')
        else:
            # dfをtsvに書き出し
            df.to_csv(path_save_local_dir + path_save_fname, sep='\t', index=False)

        # 圧縮
        if compression:
            cmd = 'gzip -f %s' % (path_save_local_dir + path_save_fname)  # -f 上書き確認なし
            subprocess.getoutput(cmd)

        # to gcs
        cmd = 'gsutil cp "{l_file}" "{r_file}"'.format(l_file=path_save_local_dir + path_save_fname + gz,
                                                       r_file=gcs_save_fname)
        r = subprocess.getoutput(cmd)

        if not re.search('Operation completed', r):
            raise ApiExecutionError(r)
        return True

    def add_tail(self, word, add_str):
        # 文字整形　string（スラッシュ等）を末尾に付ける
        pattern = re.compile(add_str + "$")
        if not pattern.search(word):
            word = word + add_str
        return word

    # ローカルディレクトリ内のfile数カウント
    def file_count(self, path_dir, pattern):
        '''

        :param string dir:　localディレクトリパス
        :param string pattern: フィルター
        :return:
        '''
        files = os.listdir(path_dir)
        count = 0
        for file in files:
            idx = re.search(pattern, file)
            if idx:
                count = count + 1
        return count

    # ファイルのダウンロード
    def download_file(self, download_file, local_save_file):

        try:
            cmd = 'gsutil -m cp "%s" "%s"' % (download_file, local_save_file)
            r = subprocess.getoutput(cmd)
            if not re.search('Operation completed', r):
                raise (ApiExecutionError(r))
        except:
            error_print()
        finally:
            return True if r else False

    # ファイルのダウンロード todo再帰的に取得できてない
    def download_file2(self, download_file, local_save_file, bucket_name=None):
        '''

        :param download_file: dir/aaa.txt
        :param local_save_file:  /tmp/aaa.txt
        :return:
        '''
        print('■', download_file, local_save_file)
        bucket_name = bucket_name if bucket_name else self.bucket_name
        try:
            # ファイル取得
            bucket = self.get_bucket(bucket_name)
            key = bucket.get_key(download_file)
            r = key.get_contents_to_filename(local_save_file)
        except:
            error_print()
        finally:
            return True if r else False

    # フォルダのダウンロード★
    def download_dir(self, download_dir, save_dir):
        '''
        save /tmp/model
        :param download_dir:gs://bucket_name/model
        :param save_dir:/tmp ※フォルダパスに注意ひとつ上のフォルダを指定すること
        :return:
        '''
        try:
            cmd = 'gsutil -m cp -r "%s" "%s"' % (download_dir, save_dir)
            r = subprocess.getoutput(cmd)
            if not re.search('Operation completed', r):
                raise (ApiExecutionError(r))
        except:
            error_print()
        finally:
            return True if r else False

    def download_dir2(self, download_dir, save_dir):
        # フォルダ名整形
        # 1 スラッシュを末尾に付ける
        save_dir = self.add_tail(save_dir, "/")
        if not re.search('/$', download_dir):
            download_dir = download_dir + '/'  # スラッシュ付与

        if not re.search('/$', save_dir):
            save_dir = save_dir + '/'  # スラッシュ付与

        # ファイル一覧取得
        prefix = download_dir.replace('gs://{bucket_name}/'.format(bucket_name=self.bucket_name), '')
        files = self.get_file_name(prefix, self.bucket_name)

        # local側のディレクトリ作成

        # フォルダがあるなら削除
        if os.path.isdir(save_dir):
            shutil.rmtree(save_dir)

        # フォルダ作成
        os.mkdir(save_dir)

        # ファイル取得
        bucket = self.get_bucket(self.bucket_name)
        if os.path.isdir(save_dir):
            for file in files:
                if file != download_dir:
                    key = bucket.get_key(file)
                    key.get_contents_to_filename(save_dir + file.split("/")[-1])
                    # print( "[F]%s Download" % (file))

        # ファイル数チェック
        local_file_ct = self.file_count(save_dir, "")
        if len(files) == local_file_ct:
            check_ct = True
        else:
            print("ダウンロード件数がマッチしない lemote:%s ,local:%s" % (len(files), local_file_ct))

        return True if check_ct else False

    # # ファイルの削除
    # def delete_file(self, path_file):
    #     bucket = self.get_bucket(self.bucket_name)
    #     r = bucket.delete_key(path_file)
    #     return True if r is None else False
    # objectの削除
    def delete_object(self, path_object):
        '''
        :param path_object: 'work/aaa.tsv'
        :return:
        '''
        """Deletes a blob from the bucket."""
        client = self.client()
        bucket = client.get_bucket(self.bucket_name)
        try:
            blob = bucket.blob(path_object)
            blob.delete()
            return path_object
        except:
            pass

    # 複数のobjectを削除
    def delete_objects(self, path_objects):
        '''
        :param path_objects: ['work/aaa.tsv','work/bbb.tsv']
        :return:
        '''
        p = MyPool(multiprocessing.cpu_count() * 10)
        delete_files = p.bucket(self.delete_object, path_objects)
        print('delete object number:%s' % len(delete_files))
        return delete_files

    # フォルダの削除
    def delete_dir(self, path_dir):
        '''
        :param path_dir: gs://bucket/work
        :return:
        '''

        # 存在確認
        if not self.check_dir(path_dir): return True

        # if not re.search('/$', path_dir):
        #     path_dir = path_dir + '/'  # スラッシュ付与

        cmd = 'gsutil rm -r "{path_dir}"'.format(path_dir=path_dir)
        r = subprocess.getoutput(cmd)

        if not re.search('Operation completed', r):
            raise ApiExecutionError(r)
        return True

        # bucket = self.get_bucket(self.bucket_name)
        #
        # # ファイル一覧取得
        # files = self.get_file_name(path_dir)
        # # print("削除フォルダ/ファイル")
        # # op(files)
        #
        # # ファイル削除
        # [bucket.delete_key(file) for file in files]
        #
        # # フォルダ削除
        # if self.check_dir(path_dir):
        #     r = bucket.delete_key(path_dir)
        #     return True if r is None else False

    # ファイル存在チェック
    def check_file(self, path_file, bucket_name=None):
        if not bucket_name:
            bucket_name = self.bucket_name
        bucket = self.client().bucket(bucket_name)
        blob = bucket.get_blob(path_file)  # object meta
        return True if blob else False

    # フォルダ存在チェック
    def check_dir(self, path_dir, bucket_name=None):
        if not re.search('/$', path_dir):
            path_dir = path_dir + '/'  # スラッシュ付与

        bucket_name = bucket_name if bucket_name else self.bucket_name

        # １つ上の階層
        check_dir_1up = path_dir[:-len(path_dir.split("/")[-2]) - 2]

        prefix = check_dir_1up.replace('gs://' + bucket_name, '') + '/'
        prefix = re.sub(r'^/', '', prefix)

        # ファイル一覧取得
        files = self.get_file_name(prefix, bucket_name)

        # files = self.get_file_name(check_dir_1up, self.bucket_name)
        # print(path_dir)
        # op(files)
        filter_check_dir = re.sub(r'^gs://%s/' % bucket_name, "", path_dir)
        for f in files:
            if re.search(filter_check_dir, f):
                return True
        return False

    # def get_file_name(self, filter_name):
    #     # 先頭のスラッシュを除く
    #     filter_name = re.sub(r'^gs://%s/' % self.bucket_name, "", filter_name)
    #
    #     pattern = re.compile(filter_name)
    #     bucket = self.get_bucket(self.bucket_name)
    #
    #     # ファイル一覧
    #     files = [key.name for key in bucket if pattern.search(key.name)]
    #     return files

    def get_file_name(self, prefix, bucket_name=None, delimiter=None):
        '''

        :param bucket_name:
        :param prefix: 先頭 work/
        :param delimiter:末尾 .tsv
        :return:
        '''
        """Lists all the blobs in the bucket that begin with the prefix.

        This can be used to list all blobs in a "folder", e.g. "public/".

        The delimiter argument can be used to restrict the results to only the
        "files" in the given "folder". Without the delimiter, the entire tree under
        the prefix is returned. For example, given these blobs:

            /a/1.txt
            /a/b/2.txt

        If you just specify prefix = '/a', you'll get back:

            /a/1.txt
            /a/b/2.txt

        However, if you specify prefix='/a' and delimiter='/', you'll get back:

            /a/1.txt

        """

        client = self.client()
        bucket_name = bucket_name if bucket_name else self.bucket_name
        blobs = client.list_blobs(bucket_name, prefix=prefix,
                                  delimiter=delimiter)

        file_list = []

        if delimiter:
            pass
        else:
            try:
                for blob in blobs:
                    file_list.append(blob.name)
            except:
                pass

        return file_list

    def get_last_file_name(self, prefix, bucket_name=None, delimiter=None, type_sort='create', desc=True):
        '''
        最後に作成or更新されたobjを取得
        :param bucket_name:
        :param prefix: 先頭 work/
        :param delimiter:末尾 .tsv
        :param type_sort:create_time or update_time
        :param desc:bool
        :return:
        '''

        client = self.client()
        bucket_name = bucket_name if bucket_name else self.bucket_name
        blobs = client.list_blobs(bucket_name, prefix=prefix,
                                  delimiter=delimiter, )

        file_list = []
        time_list = []
        last_file = None
        last_time = None
        if delimiter:
            pass
        else:
            try:
                for blob in blobs:
                    file_list.append(blob.name)
                    _time = blob.time_created if type_sort == 'create' else blob.updated
                    time_list.append(_time)
                last_time = sorted(time_list, reverse=desc)[0]
                index = time_list.index(last_time)
                last_file = file_list[index]

            except:
                pass

        return last_file, last_time

    def get_object_content(self, bucket_name, fname):

        client = storage.Client(self.project)
        # バケット名を指定してbucketを取得
        bucket = client.get_bucket(bucket_name)

        # バケット名を指定してbucketを取得
        # bucket = self.get_bucket(bucket_name)

        # Blobを作成
        blob = storage.Blob(fname, bucket)
        content = blob.download_as_string().decode()
        # a = content.split('\n')
        return content

    """
    gzipファイルを解凍
    @param path_dir 対象ディレクトリ
    @param filename ファイル名
    """

    def decompress(self, path_dir, filename):

        decompress_file = str(filename).replace('.gz', '')

        with gzip.open(path_dir + filename) as f_in:
            with open(path_dir + decompress_file, 'w') as f_out:
                f_out.writelines(f_in)

        return decompress_file

    """
    タイムゾーン変換
    @param sTimestamp タイムスタンプ(文字列)
    """

    def convert_timzone(self, sTimestamp):

        dTimestamp = dt.datetime.strptime(sTimestamp, '%Y-%m-%d %H:%M:%S')

        if self.timezone == 'jst':
            return dTimestamp + dt.timedelta(hours=-9)

        return dTimestamp


if __name__ == "__main__":
    gs = OpGcs(project, 'keep28day')

    # #Move file
    # from_file = 'aaa/bbb/f02000000000000.gz'
    # to_file = 'aaa/bbb/f02000000000000000.gz'
    # print(gs.movefile(from_file, to_file))
    #
    # ファイルのアップロード
    # local_file = '/tmp/bigquery_upload/genre_source.genre_review_fix/2000-11-05_genre_source.genre_review_fix.tsv'
    # gs_save_dir = 'gs://keep28day/genre/'
    # gs_save_filename = 'genre_review_fix'
    # print(gs.upload_file(local_file, gs_save_dir, gs_save_filename))

    # # ファイルのアップロード
    # df = pd.DataFrame({'col1': [1, 2, 3],
    #                    'col2': ["a", "b", "c"],
    #                    'col3': [111, 222, 333]
    #                    })
    # gcs_save_dir = "gs://keep28day/work/"
    # prefix = 'ddd'
    # print(gs.df_to_gcs(df, gcs_save_dir, prefix='aaa', gs_save_filename='', compression=False))

    # #ファイルのダウンロード
    # download_file = 'gs://bunseki_work/datastore/import/bunseki/custom.tsv.gz'
    # save_file = '/tmp/000aaa.gz'
    # print(gs.download_file(download_file, save_file))

    # #フォルダのダウンロード
    # download_dir = 'gs://bunseki_work/aaa'
    # save_dir = '/tmp/'
    # print(gs.download_dir(download_dir, save_dir))
    #
    # #ファイルの削除
    # path_file = 'aaa/bbb/file'
    # print(gs.delete_file(path_file))
    #
    # #フォルダの削除
    # path_dir = 'gs://bucket/subdir'
    # print(gs.delete_dir(path_dir))

    # 複数のobjectを削除
    # path_objects = ["work/y/111", "work/y/333"]
    # gs.delete_objects(path_objects)

    # #ファイル存在チェック
    # path_file = 'work/1aaa_20201023_100726.tsv'
    # print(gs.check_file(path_file))
    #
    # #フォルダ存在チェック
    check_dir = 'bigquery_upload/scrape/genre/item/insert/'
    print(gs.check_dir(check_dir, bucket_name='keep90day'))
    #
    # #ファイル一覧取得
    # prefix = 'work/a/'
    # print(gs.get_file_name(prefix, bucket_name='keep28day'))
    #
    # # last ファイル一覧取得
    # prefix = 'work/'
    # print(gs.get_last_file_name(prefix, bucket_name='keep28day', type_sort='create'))

    # # object取得
    # filter_name = "test.model_vec"
    # obj = gs.get_object_content(bucket_name='keep28day', fname=filter_name)
    # print(obj)
