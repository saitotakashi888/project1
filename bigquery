# coding: utf-8
# google BQ操作
'''
pip install pandas-gbq
pip install bigquery-python
#insert_dfで必要
conda install pyarrow fastparquet

google.cloud
https://cloud.google.com/bigquery/docs/python-client-migration?hl=ja#table-create
https://google-cloud-python.readthedocs.io/en/0.32.0/bigquery/usage.html
pip install --upgrade --ignore-installed google-cloud-bigquery[bqstorage,pandas]


# NOW UTC
TIMESTAMP_TRUNC(CURRENT_TIMESTAMP(), SECOND, 'UTC') as created_at

# timestampJSTをUTCに変換
TIMESTAMP_ADD(timestamp_col, INTERVAL -9 HOUR) AS utc
TIMESTAMP_TRUNC(TIMESTAMP_ADD(last_update, INTERVAL 0 HOUR), SECOND, 'UTC') AS created_at,

gcloud init
gcloud auth activate-service-account email --key-file= json

table to partition table
bq query --use_legacy_sql=false --allow_large_results --replace --time_partitioning_field=created_at --destination_table=genre_source.tb1 "SELECT * FROM genre_source.tb2"

avro install
pip install fastavro pandavro

'''
import re
import os
import pandavro

import configparser
import subprocess
import json
import pandas as pd
from bigquery import JOB_SOURCE_FORMAT_CSV
from bigquery import JOB_SOURCE_FORMAT_NEWLINE_DELIMITED_JSON

from bigquery import get_client
from op_file import *
from op_date import *
from op_gcs import *

from google.cloud import bigquery
from urllib.error import HTTPError, URLError
from bigquery.errors import BigQueryTimeoutException

# from BQCustomPlugin.operators.custom_gcs_to_bq import GoogleCloudStorageToBigQueryOperator
# from google.oauth2.service_account import Credentials
from google.oauth2 import service_account
import warnings

# insert_dfでスキーマを指定しない場合のwarningを非表示にする
# UserWarning: Unable to determine type of column 'aaaa'.   warnings.warn(u"Unable to determine type of column '{}'.".format(column))
warnings.simplefilter('ignore', UserWarning)
import datetime as dt

# config-------------------------------------------------------------
inifile = configparser.ConfigParser()
inifile.read(os.environ.get('PATH_SCRIPT_ROOT') + "/conf/conf.ini")

# bq
key_file_path = os.environ.get('PATH_SCRIPT_ROOT') + '/' + inifile.get("common", "GOOGLE_APPLICATION_CREDENTIALS")
timeout = 3600

# gcs
bucket_name = inifile.get('gcs', 'bucket_name')


class BigQuery:

    def __init__(self, project):
        self.project = project

    # connect bigquery
    def client(self):
        client = get_client(
            project_id=self.project,
            json_key_file=key_file_path,
            readonly=False)
        return client

    # connect bigquery google.cloud用
    def client_gcloud(self, project=None):
        set_project = project if project else self.project
        client = bigquery.Client.from_service_account_json(key_file_path, project=set_project)
        return client

        # partition table に変換

    def copy_table(self, tb_src, tb_save, drop=False):
        if drop:
            # drop source table
            self.drop(tb_save)

        # copy table
        cmd = """bq cp -f {tb_src} {tb_save}""".format(tb_src=tb_src, tb_save=tb_save)
        r = self.bq(cmd)
        return r

    def to_partition_tb(self, table, time_partitioning_field, project=None):
        # パーティションが削除されるまでの時間（秒単位）
        time_partitioning_expiration = 86400 * 365 * 10
        set_project = project if project else self.project

        # partition 一時tb作成
        tmp_table = 'dataset1.%s' % table.split('.')[1]
        self.drop(tmp_table, set_project)
        cmd = """bq query --use_legacy_sql=false \
                         --project_id={project} \
                         --allow_large_results \
                         --replace \
                         --time_partitioning_field=created_at \
                         --time_partitioning_expiration {time_partitioning_expiration} \
                         --destination_table={tmp_table} \
                        'SELECT * FROM {save_table}'
                        """.format(time_partitioning_field=time_partitioning_field, time_partitioning_expiration=time_partitioning_expiration,
                                   project=set_project, save_table=table, tmp_table=tmp_table)
        r = self.bq(cmd)

        if r:
            # drop source table
            self.drop(table)

            # copy table
            cmd = """bq cp -f {tmp_table} {save_table}""".format(save_table=table, tmp_table=tmp_table)
            r = self.bq(cmd)
            print('create to_partition_tb')
            return r

    # df → gcs JSONファイルUP → BQ
    def insert_df_from_gcs(self,
                           df,
                           table,
                           bucket_name=bucket_name,
                           fg_truncate=False,
                           autodetect=True,  # 自動スキーマ作成
                           fieldDelimiter='\t',
                           skipLeadingRows=None,  # スキップするヘッダー行
                           maxBadRecords=None,
                           allowQuotedNewlines=True,  # 引用符で囲まれた改行を許可する
                           nullMarker=None,
                           allowJaggedRows=None,
                           ignoreUnknownValues=None,
                           quote=None,
                           encoding='UTF-8',
                           project=None,
                           sync=True,  # True:同期処理
                           save_file_prefix=''
                           ):

        set_project = project if project else self.project
        client = self.client_gcloud()

        path_save_dir = '/tmp/bigquery_upload/'
        path_save_fname = '%s.tsv' % table
        gcs_bucket_name = bucket_name
        gcs_save_file = 'gs://%s/bigquery_upload/%s%s.tsv' % (gcs_bucket_name, save_file_prefix, table)

        create_dir(path_save_dir)

        # 同名のtsvが存在する場合は削除
        delete_file(path_save_dir + path_save_fname)
        ### delete_file(path_save_dir + path_save_fname + '.gz')

        # dfをtsvに書き出し
        ### df.to_json(path_save_dir + path_save_fname, orient='records', lines=True)
        df.to_csv(path_save_dir + path_save_fname, sep='\t', index=False)

        # 圧縮 圧縮された CSV データを BigQuery に読み込む場合は、圧縮されていないデータを読み込むよりも時間がかかる
        # subprocess.getoutput('gzip %s' % (path_save_dir + path_save_fname))

        # gcsにアップロード
        gcs = OpGcs(set_project, gcs_bucket_name)
        r = gcs.upload_file(local_file=path_save_dir + path_save_fname, gs_save_dir='gs://%s/bigquery_upload/' % (gcs_bucket_name), gs_save_filename=path_save_fname)
        if not r: raise ValueError('[E]gcsへのファイルuploadに失敗')

        dataset_ref = client.dataset(
            dataset_id=table.split('.')[0],
            project=set_project
        )
        table_ref = dataset_ref.table(table.split('.')[1])

        # https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv?hl=ja
        job_config = bigquery.LoadJobConfig()
        job_config.autodetect = autodetect
        job_config.source_format = bigquery.SourceFormat.CSV
        job_config.skipLeadingRows = skipLeadingRows
        job_config.fieldDelimiter = fieldDelimiter
        job_config.maxBadRecords = maxBadRecords
        job_config.allowQuotedNewlines = allowQuotedNewlines
        job_config.nullMarker = nullMarker
        job_config.allowJaggedRows = allowJaggedRows
        job_config.ignoreUnknownValues = ignoreUnknownValues
        job_config.quote = quote
        job_config.encoding = encoding

        if fg_truncate:
            job_config.write_disposition = 'WRITE_TRUNCATE'

        query_job = client.load_table_from_uri(
            gcs_save_file, table_ref, job_config=job_config
        )

        print("  Starting job {}".format(query_job.job_id))

        # wait 同期処理
        if sync:
            query_job.result()  # Waits for table load to complete.

            assert query_job.state == 'DONE'
            return query_job.output_rows, query_job

    def insert_df(self, df, table,
                  job_id=None,
                  job_id_prefix=None,
                  location=None,
                  fg_truncate=False,  # true:テーブル置換 false:追加
                  project=''):
        '''
        __index_level_0__ が挿入される場合は下記でindexを振り直す
        df=df.reset_index(drop=True)
        '''
        set_project = project if project else self.project
        client = self.client_gcloud()

        job_config = bigquery.LoadJobConfig(
            # Specify a (partial) schema. All columns are always written to the
            # table. The schema is used to assist in data type definitions.
            # schema=[
            # Specify the type of columns whose type cannot be auto-detected. For
            # example the "title" column uses pandas dtype "object", so its
            # data type is ambiguous.
            # bigquery.SchemaField("title", bigquery.enums.SqlTypeNames.STRING),
            # Indexes are written if included in the schema by name.
            # bigquery.SchemaField("wikidata_id", bigquery.enums.SqlTypeNames.STRING),
            # ],
            # Optionally, set the write disposition. BigQuery appends loaded rows
            # to an existing table by default, but with WRITE_TRUNCATE write
            # disposition it replaces the table with the loaded data.
            write_disposition="WRITE_APPEND",
        )
        if fg_truncate:
            job_config.write_disposition = 'WRITE_TRUNCATE'

        # 保存先
        dataset_ref = client.dataset(
            dataset_id=table.split('.')[0],
            project=set_project
        )

        table_ref = dataset_ref.table(table.split('.')[1])

        query_job = client.load_table_from_dataframe(df,
                                                     destination=table_ref,
                                                     # num_retries=_DEFAULT_NUM_RETRIES,
                                                     job_id=job_id,
                                                     job_id_prefix=job_id_prefix,
                                                     location=location,
                                                     project=set_project,
                                                     job_config=job_config,
                                                     # parquet_compression="snappy",
                                                     ).result()
        print("  Starting job {}".format(query_job.job_id))

        assert query_job.state == 'DONE'
        return query_job.output_rows, query_job

    # Insert or Create(新規作成 or 追加) デフォルトは追加
    def insert(self, sql, table, fg_truncate=False, project='', location="asia-northeast1"):
        '''
        # テーブルがないなら新規作成される
        https://cloud.google.com/bigquery/docs/writing-results?hl=ja
        :param sql:
        :param table: 挿入先テーブル dataset.table
        :param fg_truncate: tableが存在する場合はテーブルのmeta以外を削除
        :return:
        '''
        set_project = project if project else self.project

        client = self.client_gcloud()
        job_config = bigquery.QueryJobConfig()

        # truncate
        job_config.write_disposition = 'WRITE_APPEND'  # 追加
        if fg_truncate:
            job_config.write_disposition = 'WRITE_TRUNCATE'

        # 保存先
        dataset_ref = client.dataset(
            dataset_id=table.split('.')[0],
            project=set_project
        )
        job_config.destination = dataset_ref.table(table.split('.')[1])

        # param
        job_config.allow_large_results = True

        # クエリ実行
        query_job = client.query(sql, job_config=job_config, location=location)
        print("  Starting job {}".format(query_job.job_id))

        query_job.result(timeout=timeout)  # Waits for job to complete.

        assert query_job.state == 'DONE'
        return query_job._query_results.total_rows, query_job

    # dataframeで返す
    def select_df(self, sql,
                  job_config=None,
                  job_id=None,
                  job_id_prefix=None,
                  location=None,
                  retry=None,
                  project=None
                  ):

        set_project = project if project else self.project
        client = self.client_gcloud()

        df = client.query(sql,
                          job_config=job_config,
                          job_id=job_id,
                          job_id_prefix=job_id_prefix,
                          location=location,
                          # retry=DEFAULT_RETRY,
                          project=set_project).to_dataframe(create_bqstorage_client=True)
        return df

    # gcs経由でdataframeを返す
    def select_df_gcs(self,
                      sql,
                      bucket_name='keep1day',
                      split=True,  # True 1GBを超える場合ファイルを分割する 分割時の順序性は未確認
                      project=None,
                      prefix='',

                      ):
        '''
        データ取得時の並び順がコンソールでsqlを叩く場合やselect_dfとは異なる
        型が正しく取得できない
        :param sql:
        :param bucket_name:
        :param split:
        :param project:
        :param prefix:保存テーブルとファイルの先頭につける文字

        :return:
        '''
        set_project = project if project else self.project
        gcs = OpGcs(set_project, bucket_name)

        fname = prefix + 'bq_export*.deflate' if split else prefix + 'bq_export.deflate'
        tmp_table = "dataset1.{prefix}select_df_gcs_{now}".format(prefix=prefix, now=dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S_%f'))  # 一時テーブル名
        # tmp_table = "dataset1.select_df_gcs_2020-10-28_10-35-47"

        # export to gcs
        path_gcs_dir = "gs://{bucket_name}/from_bq/{tmp_table}".format(bucket_name=bucket_name, tmp_table=tmp_table)
        self.gcs_export(path_gcs_dir + '/' + fname, tmp_table, sql=sql, project=project, destination_format='AVRO')

        # download from gcs to local
        local_save_dir = '/tmp/gcs_export'.format(tmp_table=tmp_table)
        # delete_r_dir(local_save_dir)
        create_dir(local_save_dir)
        gcs.download_dir(path_gcs_dir, local_save_dir)

        # read avro
        df = pd.DataFrame()
        path_files = all_list_in_dir(local_save_dir + '/' + tmp_table)
        for path_file in path_files:
            df = df.append(pandavro.read_avro(path_file))

        return df

    # tableの存在チェック
    def check_table(self, table, project=None):
        '''
        :param table:
        :return: True:テーブルが存在する False:テーブルが存在しない
        '''
        set_project = project if project else self.project

        client = self.client()
        return client.check_table(table=table.split('.')[1], dataset=table.split('.')[0], project_id=set_project)

    def query(self, sql, location=None, job_config=None, job_id=None, job_id_prefix=False, project=None):
        set_project = project if project else self.project
        client = self.client_gcloud()

        query_job = client.query(sql,
                                 job_config=job_config,
                                 job_id=job_id,
                                 job_id_prefix=job_id_prefix,
                                 location=location,
                                 # retry=DEFAULT_RETRY,
                                 project=set_project)

        print("  Starting job {}".format(query_job.job_id))
        query_job.result()  # Waits for table load to complete.

        assert query_job.state == 'DONE'
        return query_job

    # drop
    def drop(self, table, project=''):
        set_project = project if project else self.project
        self.client_gcloud(set_project).delete_table(table, not_found_ok=True)
        return True

        # return self.client().delete_table(table=table.split('.')[1], dataset=table.split('.')[0], project_id=set_project)

    # truncate
    def truncate(self, table):
        query = "SELECT * FROM `%s` LIMIT 0" % (table)
        insert_rows, query_job = self.insert(query, table, fg_truncate=True)
        return query_job

    # GCS export
    def gcs_export(self,
                   destination_uri,
                   table,
                   location='asia-northeast1',
                   sql=False,
                   project=None,
                   destination_format='CSV',  # CSV AVRO ※JSONはintがstringになるため使用しないこと
                   sync=True):
        '''
        tableをgcsにエクスポート
        queryからエクスポートする場合はsqlも指定する
        :param sql: select文からtableを作成する
        :return:
        https://cloud.google.com/bigquery/docs/exporting-data?hl=ja
        '''
        client = self.client_gcloud()
        set_project = project if project else self.project

        # 一時テーブルを作成 sql → create table → export用
        if sql:
            # 初期化 一時テーブル削除
            if self.check_table(table):
                delete_r = self.drop(table)
                if not delete_r:
                    print("temp table delete error")
                # else:
                #     print("[F]temp table drop")

            insert_rows, query_job = self.insert(sql, table)
            if not query_job:
                print("temp table create error")

        dataset_ref = client.dataset(
            dataset_id=table.split('.')[0],
            project=set_project
        )
        table_ref = dataset_ref.table(table.split('.')[1])

        job_config = bigquery.ExtractJobConfig()

        job_config.destination_format = destination_format
        if destination_format == 'CSV':
            job_config.compression = 'GZIP'
            job_config.field_delimiter = '\t'
        elif destination_format == 'AVRO':
            job_config.compression = 'DEFLATE'

        query_job = client.extract_table(
            table_ref,
            [destination_uri],
            location=location,
            job_config=job_config
        )  # API request
        query_job.result()  # Waits for job to complete.

        print("  Starting job {}".format(query_job.job_id))

        # wait 同期処理
        if sync:
            query_job.result()  # Waits for table load to complete.

            assert query_job.state == 'DONE'
            return query_job

    # GCS import
    def gcs_import(self,
                   table,
                   gcs_save_file,  # 複数ファイルの場合はワイルドカード指定 gs://mybucket/mydata*.gz
                   file_type='tsv',  # csv,tsv,json
                   location='asia-northeast1',
                   fg_truncate=False,
                   schema=None,
                   autodetect=True,  # 自動スキーマ作成
                   fieldDelimiter='\t',
                   skipLeadingRows=1,  # スキップするヘッダー行
                   maxBadRecords=None,
                   allowQuotedNewlines=True,  # 引用符で囲まれた改行を許可する
                   nullMarker=None,
                   allowJaggedRows=True,
                   ignoreUnknownValues=True,
                   quote=None,
                   encoding='UTF-8',
                   project=None,
                   sync=True  # True:同期処理
                   ):

        '''
        import dataが0行の場合下記エラーになる
        google.api_core.exceptions.BadRequest: 400 Error while reading data, error message:
        CSV table encountered too many errors, giving up. Rows: 1; errors: 1. Please look into the errors[] collection for more details.

        schema
        https://cloud.google.com/bigquery/docs/schemas?hl=ja
        mode NULLABLE, REQUIRED , REPEATED
        1.tsv用のschema
        schema = pd.DataFrame(
                [
                    ["id", "INTEGER", "REQUIRED"],
                    ["name", "STRING", "REQUIRED"],
                    ["updated_at", "TIMESTAMP", "REQUIRED"],
                ],
                columns=['name', 'type', 'mode'],
            )
        
        2.json用のschema
        schema = [
            bigquery.SchemaField('col1', 'STRING', mode='NULLABLE'),
            bigquery.SchemaField('col2', 'STRING', mode='NULLABLE'),
            bigquery.SchemaField('col3', 'STRING', mode='REPEATED'), #配列
            bigquery.SchemaField('col4', 'RECORD', mode='REPEATED',
                                 fields=(
                                     bigquery.SchemaField('aaa', 'STRING'),
                                     bigquery.SchemaField('bbb', 'STRING'),
                                     bigquery.SchemaField('ccc', 'STRING'),
                                     )
                                 ),
            bigquery.SchemaField('updated_at', 'TIMESTAMP', mode='REQUIRED'),
        ]
        '''

        set_project = project if project else self.project
        client = self.client_gcloud()

        dataset_ref = client.dataset(
            dataset_id=table.split('.')[0],
            project=set_project
        )
        table_ref = dataset_ref.table(table.split('.')[1])

        # https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv?hl=ja
        job_config = bigquery.LoadJobConfig()

        job_config.max_bad_records = maxBadRecords
        job_config.null_marker = nullMarker
        job_config.allow_jagged_rows = allowJaggedRows
        job_config.ignore_unknown_values = ignoreUnknownValues
        job_config.quote = quote
        job_config.encoding = encoding
        job_config.allow_quoted_newlines = allowQuotedNewlines

        if schema is not None:
            if file_type in ('csv', 'tsv'):
                schema_edit = []
                for col, v in schema.iterrows():
                    schema_edit.append(bigquery.SchemaField(v["name"], v["type"], v["mode"]))
                job_config.schema = schema_edit
            elif file_type in ('json'):
                job_config.schema = schema

        else:
            job_config.autodetect = autodetect

        if file_type in ('csv', 'tsv'):
            job_config.source_format = bigquery.SourceFormat.CSV
            job_config.field_delimiter = fieldDelimiter
            job_config.skip_leading_rows = skipLeadingRows

        elif file_type in ('json'):
            job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON
        #
        if fg_truncate:
            job_config.write_disposition = 'WRITE_TRUNCATE'

        query_job = client.load_table_from_uri(
            gcs_save_file, table_ref, job_config=job_config, location=location
        )

        print("  Starting job {}".format(query_job.job_id))

        # wait 同期処理
        if sync:
            query_job.result()  # Waits for table load to complete.

            assert query_job.state == 'DONE'
            return query_job.output_rows, query_job

    # table件数カウント
    def count_table(self, table):
        query = "SELECT count(*) as cnt FROM `%s`" % table
        df = self.select_df(query)
        cnt = list(df.cnt)[0]
        return cnt

    # table作成
    def create_table(self, schema, table, project):
        set_project = project if project else self.project

        client = self.client()
        if not self.check_table(table):
            query_job = client.create_table(table=table.split('.')[1], dataset=table.split('.')[0], project_id=set_project, schema=schema)
            return query_job
        else:
            print("Error:既にテーブルが存在しています")
            return False

    def get_table_schema(self, table):
        client = self.client()
        query_job = client.get_table_schema(table=table.split('.')[1], dataset=table.split('.')[0], project_id=self.project)
        return query_job

    # 集計対象カラムがが配列かをチェックし、配列カラム名と配列jsonカラムを返す
    def check_is_array_col(self, table, cols):
        array_cols = []
        array_json_cols = []
        schemas = self.get_table_schema(table)
        for schema in schemas:
            if schema['mode'] == 'REPEATED' and schema['type'] == 'STRING' and schema['name'] in cols:
                array_cols.append(schema['name'])
            elif schema['mode'] == 'REPEATED' and schema['type'] == 'RECORD' and schema['name'] in cols:
                array_json_cols.append(schema['name'])
        return array_cols, array_json_cols

    # bq
    def bq(self, cmd_str):
        query_job = subprocess.getoutput(cmd_str)
        if re.search("Error", query_job):
            raise Exception(query_job)

        else:
            return True

    def udf(self, name):
        if name == 'set':
            #             --SELECT ARRAY_STRING_INTERSECTION(['1','2','3'], ['1','2','4'])
            udf = """
                CREATE TEMPORARY FUNCTION ARRAY_STRING_INCLUDE(arr ARRAY<STRING>, val STRING)
                    RETURNS BOOL AS ((
                      SELECT LOGICAL_OR(elem = val) FROM UNNEST(arr) as elem
                ));
                --積集合
                CREATE TEMPORARY FUNCTION ARRAY_STRING_INTERSECTION(arr1 ARRAY<STRING>, arr2 ARRAY<STRING>)
                    RETURNS ARRAY<STRING> AS ((
                      SELECT ARRAY(SELECT val FROM UNNEST(arr1) AS val WHERE (ARRAY_STRING_INCLUDE(arr2, val)))
                ));  
                --差集合
                CREATE TEMPORARY FUNCTION ARRAY_STRING_DIFFERENCE(arr1 ARRAY<STRING>, arr2 ARRAY<STRING>)
                    RETURNS ARRAY<STRING> AS ((
                      SELECT ARRAY(SELECT val FROM UNNEST(arr1) AS val WHERE NOT(ARRAY_STRING_INCLUDE(arr2, val)))
                )); \n\n           
            """
        elif name == 'slice':
            # SELECT ARRAY_TO_STRING(ARRAY_SLICE(SPLIT('aaa,bbb', ','), 1, 1), '')
            # return bbb
            udf = """
            --slice
                CREATE temp  FUNCTION ARRAY_SLICE(arr ARRAY<STRING>, start INT64, finish INT64) 
                RETURNS ARRAY<STRING> AS (
                  ARRAY(
                    SELECT part FROM UNNEST(arr) part WITH OFFSET index 
                    WHERE index BETWEEN start AND finish ORDER BY index
                  )
                );
            """

        return udf


#


if __name__ == "__main__":
    project = 'project1'
    bq = BigQuery(project)

    ###partition 追加
    # bq.to_partition_tb('genre_source.tb1',)

    # # ### Select
    # sql = """SELECT * FROM `{project}.genre_source.tb1`  LIMIT 1""".format(project=project)
    # df = bq.select_df(sql)
    # print(df)
    #
    # ### Insert(新規作成 or 追加)
    # sql = """SELECT item_name FROM `{project}.genre_source.tb1` LIMIT 2""".format(project=project)
    # r = bq.insert(sql, table="work.test3", fg_truncate=False)

    ### insert_df_from_gcs
    # bq.insert_df_from_gcs(df, table='work.test', fg_drop_schema=False, fg_truncate=False, table_schema=None, project='')

    # ### Query
    # sql = """INSERT INTO  `{project}.work.test` (item_name) SELECT item_name FROM `{project}.genre_source.tb1` LIMIT 2""" \
    #     .format(project=project)
    # r = bq.query(sql)

    # ### gcs_export
    # tmp_table = "genre_source.tmp_export".format(project=project)  # 一時テーブル名
    # save_file_path = "gs://project1_export/from_bq/export*.deflate"
    # sql = """SELECT * FROM `{project}.genre_source.tb1`  LIMIT 1""".format(project=project)
    # r = self.bq.gcs_export(save_file_path, tmp_table, sql=sql, destination_format='AVRO')
    #
    # ### gcs_import
    # r = bq.gcs_import(load_file_path="gs://project1_export/from_pg/export*.gz",
    #                   table="work.aaa".format(project=project))
    #
    # ### drop パーティションテーブルのドロップはパーティションが消えるので注意
    # r = bq.drop("work.test2".format(project=project))
    #
    # ### truncate パーティションテーブルのTruncateはパーティションは消えない
    # r = bq.truncate("genre_source.tb1".format(project=project))
    #
    #
    # ### create_table
    # table = "genre_source.test_create".format(project=project)  # 一時テーブル名
    # schema = [{'mode': 'NULLABLE', 'name': 'aaa', 'type': 'STRING'},
    #           {'mode': 'NULLABLE', 'name': 'name', 'type': 'STRING'},
    #           {'mode': 'NULLABLsE', 'name': 'description', 'type': 'STRING'}
    #           ]
    # r = bq.create_table(schema, table)
    #
    # ### get_table_schema
    table = "dataset1.tb1".format(project=project)
    r = bq.get_table_schema(table)
    print(r)
    #
    # ### check_table
    # table = "genre_source.tb1".format(project=project)
    # r = bq.check_table(table)
    # print(r)
    #
    # ### count_table
    # table = "genre_source.tb1".format(project=project)
    # r = bq.count_table(table)
    # print(r)

    # ### insert_df
    # df = pd.DataFrame([["day1", "day2", "day1", "day2", "day1", "day2"],
    #                    ["A", "B", "A", "B", "C", "C"],
    #                    [100, 150, 200, 150, 100, 50],
    #                    [120, 160, 100, 180, 110, 80]]).T
    # df.columns = ["day_no", "class", "s1", "s2"]  # カラム名を付ける
    # df.index = [11, 12, 13, 14, 15, 16]  # インデックス名を付ける
    # bq.insert_df(df, 'work.test2')

    #
    # ### check_is_array_col 集計対象カラムがが配列かをチェックし、配列カラム名を返す
    # table = "genre_mart.tb1".format(project=default_project)
    # r = bq.check_is_array_col(table, ['col1'])
    # print(r)
    print('')
